model_selection

#сохранение мощности классов
sklearn.model_selection.StratifiedKFold(n_splits='warn',
 shuffle=False,
 random_state=None)
 
 sklearn.model_selection.KFold(n_splits=3, shuffle=True, random_state=None)
 
 sklearn.model_selection.TimeSeriesSplit(n_splits='warn',
 max_train_size=None)
 
 
 cross_val_score(logreg, X, y, cv=cv)



from sklearn.model_selection import validation_curve   
----- Валидационная кривая (Validation Curve) показывает зависимость качества / ошибки при 
выбранной схеме контроля от значений гиперпараметров.

train_errors, test_errors = validation_curve(model,
 X, y,
 param_name=param_name,
 param_range=pars,
 cv=cv.split(X),
 scoring='accuracy',
 n_jobs=-1)
 
 
 from sklearn.model_selection import GridSearchCV
parameters = {'metric':('euclidean', 'manhattan', 'chebyshev'),
 'n_neighbors':[1, 3, 5, 7, 9, 11] , scoring='roc_auc'}
clf = GridSearchCV(estimator, parameters, cv=5)
clf.fit(X, y)
clf.cv_results_['mean_test_score']

model.get_params()

Распараллеливание
n_jobs=-1





KNN




Linear

sklearn.linear_model.Lasso/Ridge(alpha=1.0, fit_intercept=True, normalize=False, 
precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, 
positive=False, random_state=None, selection="cyclic")


Trees

Критерий Джини и энтропийный


from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(criterion='gini', splitter='best',
 max_depth=None, min_samples_split=2,
 min_samples_leaf=1,
 min_weight_fraction_leaf=0.0,
 max_features=None, random_state=3,
 max_leaf_nodes=None, 
 min_impurity_decrease=0.0,
 min_impurity_split=None, class_weight=None,
 presort=False)
 
 
 С помощью деревьев можно вводить новые признаки. Строим маленькое дерево. И если объект попал в i лист, ставим 1 в этот признак.
 
 
 Отбор признаков
 Если признак коррелирует с другим, можно удалить










